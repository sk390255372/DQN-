# 强化学习算法对比分析报告

## 目录
1. [在线强化学习算法对比](#1-在线强化学习算法对比)
2. [算法模块对比](#2-算法模块对比)
3. [关键参数分析](#3-关键参数分析)
4. [PPO：策略梯度的改进变种](#4-ppo策略梯度的改进变种)
5. [Offline RL 对比分析](#5-offline-rl-对比分析)
6. [实验结果总结](#6-实验结果总结)

---

## 1. 在线强化学习算法对比

### 1.1 算法思想对比

| 特性 | DQN | REINFORCE | PPO |
|------|-----|-----------|-----|
| **类型** | Value-based | Policy-based | Policy-based (Actor-Critic) |
| **核心思想** | 学习动作价值函数Q(s,a)，选择Q值最大的动作 | 直接学习策略π(a\|s)，通过回报加权梯度更新 | 学习策略同时估计价值，通过裁剪限制更新幅度 |
| **探索方式** | ε-贪婪策略 | 策略本身的随机性 | 策略本身的随机性 + 熵正则化 |
| **更新方式** | 离策略(Off-policy) | 在策略(On-policy) | 在策略(On-policy) |
| **样本效率** | 高（可重用经验） | 低（只能用一次） | 中等（多次epoch更新） |

### 1.2 理论基础

#### DQN (Deep Q-Network)
- **贝尔曼方程**: $Q(s,a) = r + \gamma \max_{a'} Q(s', a')$
- **损失函数**: $L = \mathbb{E}[(r + \gamma \max_{a'} Q_{target}(s', a') - Q(s, a))^2]$
- **关键创新**: 经验回放 + 目标网络

#### REINFORCE
- **策略梯度定理**: $\nabla J(\theta) = \mathbb{E}[\nabla \log \pi_\theta(a|s) \cdot G_t]$
- **Monte Carlo回报**: $G_t = \sum_{k=0}^{T-t} \gamma^k r_{t+k}$
- **关键特点**: 无偏估计，但高方差

#### PPO (Proximal Policy Optimization)
- **裁剪目标函数**: $L^{CLIP} = \mathbb{E}[\min(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t)]$
- **概率比**: $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$
- **关键创新**: 限制策略更新幅度，保证训练稳定性

### 1.3 优缺点分析

#### DQN
**优点:**
- 样本效率高，可重复使用经验
- 适合离散动作空间
- 训练相对稳定

**缺点:**
- 不适用于连续动作空间
- 需要大量内存存储经验池
- 可能过估计Q值

#### REINFORCE
**优点:**
- 理论简洁，易于理解和实现
- 可直接处理连续动作空间
- 策略梯度无偏

**缺点:**
- 高方差，需要大量样本
- 只能在线学习，样本效率低
- 训练不稳定

#### PPO
**优点:**
- 训练稳定，收敛可靠
- 可处理连续和离散动作
- 实现相对简单，效果好

**缺点:**
- 样本效率低于DQN
- 需要调节裁剪参数
- 对超参数较敏感

---

## 2. 算法模块对比

### 2.1 网络架构

```
┌─────────────────────────────────────────────────────────────────┐
│                        DQN 网络架构                              │
├─────────────────────────────────────────────────────────────────┤
│  Input(state) → FC(64) → ReLU → FC(64) → ReLU → FC(n_actions)  │
│                                                                  │
│  输出: Q(s, a) 对每个动作的Q值                                    │
└─────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────┐
│                     REINFORCE 网络架构                           │
├─────────────────────────────────────────────────────────────────┤
│  Input(state) → FC(256) → Tanh → FC(256) → Tanh → FC(n_actions)│
│                                              ↓                   │
│                                          Softmax                 │
│  输出: π(a|s) 动作概率分布                                        │
└─────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────┐
│                       PPO 网络架构                               │
├─────────────────────────────────────────────────────────────────┤
│                    ┌─→ Actor Head → Softmax → π(a|s)            │
│  Input → Shared   │                                              │
│  FC(64) → Tanh ───┤                                              │
│  FC(64) → Tanh    │                                              │
│                    └─→ Critic Head → V(s)                        │
│                                                                  │
│  输出: 策略π(a|s) + 状态价值V(s)                                  │
└─────────────────────────────────────────────────────────────────┘
```

### 2.2 核心模块对比

| 模块 | DQN | REINFORCE | PPO |
|------|-----|-----------|-----|
| **经验存储** | ReplayBuffer (大容量) | Episode Buffer (单次) | Rollout Buffer (批量) |
| **目标计算** | TD Target + 目标网络 | Monte Carlo回报 | GAE (广义优势估计) |
| **策略选择** | ε-greedy | 采样 | 采样 |
| **更新频率** | 每步更新 | Episode结束 | 收集N步后批量更新 |
| **梯度计算** | MSE Loss反向传播 | 策略梯度 | Clipped PPO Loss |

### 2.3 代码实现对比

#### DQN 核心更新逻辑
```python
def update(self):
    # 从经验池采样
    batch = self.memory.sample(batch_size)
    states, actions, rewards, next_states, dones = batch
    
    # 计算TD目标
    with torch.no_grad():
        next_q = self.target_net(next_states).max(1)[0]
        targets = rewards + gamma * next_q * (1 - dones)
    
    # 计算当前Q值
    current_q = self.policy_net(states).gather(1, actions)
    
    # MSE损失
    loss = F.mse_loss(current_q, targets)
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()
```

#### REINFORCE 核心更新逻辑
```python
def update(self):
    # 计算折扣回报
    returns = []
    G = 0
    for r in reversed(self.rewards):
        G = r + gamma * G
        returns.insert(0, G)
    
    # 标准化回报
    returns = (returns - returns.mean()) / (returns.std() + 1e-8)
    
    # 策略梯度更新
    loss = 0
    for log_prob, G in zip(self.log_probs, returns):
        loss -= log_prob * G  # 梯度上升
    
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()
```

#### PPO 核心更新逻辑
```python
def update(self):
    # 计算GAE和returns
    advantages, returns = self.compute_gae()
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
    
    # 多epoch更新
    for _ in range(ppo_epochs):
        # 计算新旧策略比率
        new_log_probs = self.policy(states).log_prob(actions)
        ratio = torch.exp(new_log_probs - old_log_probs)
        
        # 裁剪目标
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1-eps, 1+eps) * advantages
        actor_loss = -torch.min(surr1, surr2).mean()
        
        # 价值损失
        value_loss = F.mse_loss(values, returns)
        
        # 总损失
        loss = actor_loss + 0.5 * value_loss - 0.01 * entropy
```

---

## 3. 关键参数分析

### 3.1 DQN 关键参数

| 参数 | 默认值 | 作用 | 调优建议 |
|------|--------|------|----------|
| `learning_rate` | 1e-4 | 网络学习速率 | 过大导致不稳定，过小收敛慢 |
| `gamma` | 0.99 | 折扣因子 | 接近1关注长期，接近0关注短期 |
| `epsilon_start` | 1.0 | 初始探索率 | 通常从1开始 |
| `epsilon_end` | 0.01 | 最终探索率 | 保留少量探索 |
| `epsilon_decay` | 0.995 | 探索衰减率 | 控制探索到利用的过渡速度 |
| `buffer_size` | 100000 | 经验池大小 | 越大样本多样性越好 |
| `batch_size` | 64 | 批量大小 | 影响梯度估计方差 |
| `target_update` | 10 | 目标网络更新频率 | 太频繁不稳定，太慢收敛慢 |

### 3.2 REINFORCE 关键参数

| 参数 | 默认值 | 作用 | 调优建议 |
|------|--------|------|----------|
| `learning_rate` | 1e-3 | 策略网络学习率 | 策略梯度通常需要较大学习率 |
| `gamma` | 0.99 | 折扣因子 | 同DQN |
| `hidden_size` | 256 | 隐藏层大小 | 任务简单可减小 |
| `baseline` | True | 是否使用基线 | 减少方差，建议开启 |
| `normalize_returns` | True | 回报标准化 | 减少方差，加速收敛 |

### 3.3 PPO 关键参数

| 参数 | 默认值 | 作用 | 调优建议 |
|------|--------|------|----------|
| `learning_rate` | 3e-3 | 学习率 | PPO对学习率较敏感 |
| `gamma` | 0.99 | 折扣因子 | 同上 |
| `gae_lambda` | 0.95 | GAE参数 | 接近1偏向MC，接近0偏向TD |
| `clip_eps` | 0.2 | PPO裁剪参数 | **核心参数**，限制策略变化 |
| `entropy_coef` | 0.01 | 熵正则化系数 | 鼓励探索，防止过早收敛 |
| `value_coef` | 0.5 | 价值损失系数 | 平衡actor和critic学习 |
| `ppo_epochs` | 4 | 每批数据更新次数 | 过多可能过拟合 |
| `max_grad_norm` | 0.5 | 梯度裁剪 | 防止梯度爆炸 |

### 3.4 参数敏感性分析

```
PPO clip_eps 参数影响:
┌────────────────────────────────────────────────────┐
│  clip_eps = 0.1  →  保守更新，训练慢但稳定          │
│  clip_eps = 0.2  →  标准设置，平衡稳定性和速度      │
│  clip_eps = 0.3  →  激进更新，可能不稳定            │
└────────────────────────────────────────────────────┘

GAE lambda 参数影响:
┌────────────────────────────────────────────────────┐
│  λ = 0    →  纯TD(0)，低方差高偏差                  │
│  λ = 0.95 →  平衡偏差和方差（推荐）                 │
│  λ = 1    →  纯Monte Carlo，高方差低偏差            │
└────────────────────────────────────────────────────┘
```

---

## 4. PPO：策略梯度的改进变种

### 4.1 从REINFORCE到PPO的演进

```
REINFORCE → Actor-Critic → TRPO → PPO
    │            │           │       │
    │            │           │       └─ 用裁剪代替KL约束
    │            │           └─ 添加KL散度约束
    │            └─ 添加Critic减少方差
    └─ 基础策略梯度
```

### 4.2 PPO相对于REINFORCE的改进

| 改进点 | REINFORCE | PPO | 效果 |
|--------|-----------|-----|------|
| **方差控制** | 无/简单基线 | GAE + Critic | 大幅降低方差 |
| **更新稳定性** | 无约束 | 裁剪比率 | 防止策略崩溃 |
| **样本利用** | 单次使用 | 多epoch更新 | 提高样本效率 |
| **价值估计** | 无 | 学习V(s) | 更准确的优势估计 |

### 4.3 PPO的核心创新：裁剪机制

```python
# 标准策略梯度（可能导致过大更新）
L_PG = ratio * advantage

# PPO裁剪（限制更新幅度）
L_CLIP = min(
    ratio * advantage,
    clip(ratio, 1-ε, 1+ε) * advantage
)
```

**裁剪机制的作用：**
- 当 `ratio > 1+ε` 且 `advantage > 0`：阻止过度增加该动作概率
- 当 `ratio < 1-ε` 且 `advantage < 0`：阻止过度减少该动作概率
- 保证新旧策略不会相差太大

### 4.4 实验结果对比

| 指标 | REINFORCE | PPO |
|------|-----------|-----|
| 达到475+所需episodes | ~300 | ~400 |
| 最终评估分数 | 500.00 | 500.00 |
| 训练稳定性 | 中等 | 高 |
| 实现复杂度 | 简单 | 中等 |

---

## 5. Offline RL 对比分析

### 5.1 数据集生成方式

#### 5.1.1 随机策略 (Random Policy)
```python
class RandomPolicy:
    def select_action(self, state):
        return np.random.randint(0, n_actions)
```
- **特点**: 完全随机探索，覆盖状态空间广
- **数据质量**: 低（平均episode长度 ~23步）
- **适用场景**: 测试算法对低质量数据的鲁棒性

#### 5.1.2 专家策略 (Expert Policy)
```python
class ExpertPolicy:
    def __init__(self, model_path):
        self.model = load_trained_model(model_path)
    
    def select_action(self, state):
        return self.model.act(state, deterministic=True)
```
- **特点**: 使用训练好的模型收集高质量轨迹
- **数据质量**: 高（平均episode长度 ~495步）
- **适用场景**: 模仿学习、验证算法上限

#### 5.1.3 ε-贪婪混合策略 (Epsilon-Greedy Mixed)
```python
class EpsilonGreedyPolicy:
    def __init__(self, expert_model, epsilon):
        self.expert = expert_model
        self.epsilon = epsilon
    
    def select_action(self, state):
        if np.random.random() < self.epsilon:
            return np.random.randint(0, n_actions)
        return self.expert.act(state)
```
- **特点**: 混合专家和随机动作
- **ε=0.3**: 70%专家 + 30%随机（平均~393步）
- **ε=0.5**: 50%专家 + 50%随机（平均~140步）
- **适用场景**: 模拟现实中的次优数据

### 5.2 Offline RL 算法模块

#### 5.2.1 Behavioral Cloning (BC)

```
┌─────────────────────────────────────────────────────┐
│                  Behavioral Cloning                  │
├─────────────────────────────────────────────────────┤
│  监督学习方式直接模仿数据中的动作                      │
│                                                      │
│  网络: state → FC → FC → softmax → action_probs     │
│                                                      │
│  损失: CrossEntropyLoss(predicted, actual_action)   │
│                                                      │
│  优点: 简单直接，训练快速                             │
│  缺点: 分布偏移问题，无法处理OOD状态                  │
└─────────────────────────────────────────────────────┘
```

**核心代码：**
```python
class BCPolicy(nn.Module):
    def __init__(self, state_dim, action_dim, hidden=128):
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, action_dim)
        )
    
    def forward(self, state):
        return self.net(state)

def train_bc(dataset, epochs=100):
    states, actions = dataset.get_all()
    for epoch in range(epochs):
        logits = policy(states)
        loss = F.cross_entropy(logits, actions)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

#### 5.2.2 Conservative Q-Learning (CQL)

```
┌─────────────────────────────────────────────────────┐
│              Conservative Q-Learning                 │
├─────────────────────────────────────────────────────┤
│  在标准Q-learning基础上添加保守性正则项              │
│                                                      │
│  标准TD Loss:                                        │
│    L_TD = (Q(s,a) - (r + γ max Q(s',a')))²          │
│                                                      │
│  CQL正则项:                                          │
│    L_CQL = α * (log Σ exp(Q(s,a)) - Q(s,a_data))    │
│                                                      │
│  总损失: L = L_TD + L_CQL                           │
│                                                      │
│  作用: 惩罚OOD动作的Q值，保守估计                    │
└─────────────────────────────────────────────────────┘
```

**核心代码：**
```python
def train_cql(dataset, alpha=1.0):
    states, actions, rewards, next_states, dones = dataset.sample(batch_size)
    
    # 标准TD目标
    with torch.no_grad():
        next_q = target_net(next_states).max(1)[0]
        targets = rewards + gamma * next_q * (1 - dones)
    
    current_q = q_net(states)
    q_values = current_q.gather(1, actions)
    td_loss = F.mse_loss(q_values, targets)
    
    # CQL正则项
    logsumexp_q = torch.logsumexp(current_q, dim=1).mean()
    data_q = q_values.mean()
    cql_loss = alpha * (logsumexp_q - data_q)
    
    # 总损失
    loss = td_loss + cql_loss
```

### 5.3 关键参数分析

#### BC 参数
| 参数 | 默认值 | 作用 |
|------|--------|------|
| `hidden_size` | 128 | 网络容量 |
| `learning_rate` | 1e-3 | 学习速率 |
| `epochs` | 100 | 训练轮数 |
| `batch_size` | 256 | 批量大小 |

#### CQL 参数
| 参数 | 默认值 | 作用 | 影响 |
|------|--------|------|------|
| `alpha` | 1.0 | **CQL正则化强度** | 核心参数 |
| `learning_rate` | 1e-3 | 学习速率 | |
| `gamma` | 0.99 | 折扣因子 | |
| `target_update` | 100 | 目标网络更新 | |

**Alpha参数影响：**
```
α = 0.1  →  弱保守，可能过估计OOD动作 (496.4±15.4)
α = 0.5  →  中等保守 (493.5±23.9)
α = 1.0  →  标准设置 (496.1±17.5)
α = 2.0  →  强保守，最佳效果 (497.3±13.9)
```

### 5.4 实验结果对比

#### 数据集质量影响
| 数据集 | BC得分 | CQL得分 |
|--------|--------|---------|
| Random | 36.9±16.8 | 12.4±7.9 |
| Expert | 496.1±18.0 | 494.4±22.9 |
| Mixed(ε=0.3) | 496.8±14.4 | 494.1±24.0 |
| Mixed(ε=0.5) | 494.1±21.4 | 497.3±15.2 |

#### 关键发现
1. **数据质量是关键**: 专家数据 >> 混合数据 >> 随机数据
2. **BC vs CQL**: 
   - BC在高质量数据上表现更好，实现简单
   - CQL对低质量数据更鲁棒，但计算开销大
3. **Alpha调优**: 适当增大alpha可提高CQL稳定性

---

## 6. 实验结果总结

### 6.1 在线RL算法性能

| 算法 | 100 Episodes评估得分 | 是否通过475+ |
|------|---------------------|--------------|
| DQN | ~490 | ✅ |
| REINFORCE | 500.00 | ✅ |
| PPO | 500.00 | ✅ |

### 6.2 Offline RL算法性能

| 算法 | 专家数据集 | 随机数据集 |
|------|-----------|-----------|
| BC | 496.1±18.0 | 36.9±16.8 |
| CQL | 494.4±22.9 | 12.4±7.9 |

### 6.3 算法选择建议

```
场景选择指南:
┌─────────────────────────────────────────────────────────────┐
│  离散动作 + 可交互环境  →  DQN（样本效率高）                  │
│  连续动作 + 可交互环境  →  PPO（稳定可靠）                    │
│  简单任务 + 快速原型    →  REINFORCE（实现简单）              │
│  只有离线数据 + 高质量  →  BC（简单有效）                     │
│  只有离线数据 + 混合质量 →  CQL（保守稳定）                   │
└─────────────────────────────────────────────────────────────┘
```

---

## 附录：文件结构

```
Coding/
├── agents/
│   ├── cartpole_dqn.py      # DQN实现
│   ├── reinforce.py         # REINFORCE实现
│   └── ppo_stable.py        # PPO实现
├── scores/
│   ├── reinforce_scores.csv/.png
│   └── ppo_scores.csv/.png
├── models/
│   ├── cartpole_reinforce.torch
│   └── cartpole_ppo.torch
├── offline_rl.py            # Offline RL (BC + CQL)
├── offline_rl_results.json  # Offline RL结果
└── train.py                 # 训练主脚本
```
