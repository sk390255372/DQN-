# CartPole 强化学习算法对比分析报告

## 目录
1. [算法概述](#1-算法概述)
2. [算法模块对比](#2-算法模块对比)
3. [关键参数分析](#3-关键参数分析)
4. [实验结果对比](#4-实验结果对比)
5. [非DQN变种对比分析](#5-非dqn变种对比分析)
6. [结论与建议](#6-结论与建议)

---

## 1. 算法概述

### 1.1 DQN (Deep Q-Network) - 基线算法

**核心思想**：使用神经网络近似Q函数，通过最小化TD误差来学习最优动作价值函数。

**算法特点**：
- 基于价值函数（Value-based）
- 使用经验回放（Experience Replay）打破数据相关性
- 使用目标网络（Target Network）稳定训练
- ε-贪婪策略进行探索

**更新公式**：
```
Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]
```

### 1.2 REINFORCE - 蒙特卡洛策略梯度

**核心思想**：直接优化策略参数，使用完整episode的回报进行蒙特卡洛估计。

**算法特点**：
- 基于策略（Policy-based）
- 蒙特卡洛方法，需要完整episode
- 高方差，可使用基线(baseline)减少
- 无需经验回放

**策略梯度公式**：
```
∇J(θ) = E[Σ_t (G_t - b) ∇log π(a_t|s_t; θ)]
```

### 1.3 A2C (Advantage Actor-Critic)

**核心思想**：结合策略梯度和价值函数，使用优势函数减少方差。

**算法特点**：
- Actor-Critic架构（同时学习策略和价值函数）
- 使用优势函数 A(s,a) = Q(s,a) - V(s)
- 可以每步更新（比REINFORCE更高效）
- 支持n-step回报

**损失函数**：
```
L = L_actor + c1 * L_critic + c2 * L_entropy
L_actor = -E[A(s,a) * log π(a|s)]
L_critic = E[(V(s) - R)²]
L_entropy = -E[H(π)]
```

---

## 2. 算法模块对比

### 2.1 网络架构对比

| 模块 | DQN | REINFORCE | A2C |
|------|-----|-----------|-----|
| **网络类型** | Q网络 | 策略网络 | Actor + Critic |
| **输入** | 状态 s | 状态 s | 状态 s |
| **输出** | Q(s,a) for all a | π(a\|s) | π(a\|s), V(s) |
| **网络数量** | 2 (主网络+目标网络) | 1 | 2 (可共享底层) |

### 2.2 核心组件对比

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           DQN 核心组件                                   │
├─────────────────────────────────────────────────────────────────────────┤
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐              │
│  │  Q-Network   │    │ Target Net   │    │ Replay Buffer│              │
│  │  Q(s,a;θ)    │    │  Q(s,a;θ⁻)   │    │   (s,a,r,s') │              │
│  └──────────────┘    └──────────────┘    └──────────────┘              │
│         ↓                   ↓                   ↓                       │
│  ┌─────────────────────────────────────────────────────┐               │
│  │          TD Loss = (r + γ max Q(s',a';θ⁻) - Q(s,a;θ))²              │
│  └─────────────────────────────────────────────────────┘               │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│                        REINFORCE 核心组件                                │
├─────────────────────────────────────────────────────────────────────────┤
│  ┌──────────────┐    ┌──────────────┐                                   │
│  │ Policy Net   │    │  Episode     │                                   │
│  │  π(a|s;θ)    │    │  Trajectory  │                                   │
│  └──────────────┘    └──────────────┘                                   │
│         ↓                   ↓                                           │
│  ┌─────────────────────────────────────────────────────┐               │
│  │     Policy Loss = -Σ (G_t - baseline) * log π(a_t|s_t)              │
│  └─────────────────────────────────────────────────────┘               │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│                          A2C 核心组件                                    │
├─────────────────────────────────────────────────────────────────────────┤
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐              │
│  │    Actor     │    │   Critic     │    │  n-step      │              │
│  │   π(a|s;θ)   │    │   V(s;φ)     │    │  Trajectory  │              │
│  └──────────────┘    └──────────────┘    └──────────────┘              │
│         ↓                   ↓                   ↓                       │
│  ┌─────────────────────────────────────────────────────┐               │
│  │  Advantage A = R - V(s)                                             │
│  │  Actor Loss = -A * log π(a|s) - β * H(π)                            │
│  │  Critic Loss = (V(s) - R)²                                          │
│  └─────────────────────────────────────────────────────┘               │
└─────────────────────────────────────────────────────────────────────────┘
```

### 2.3 学习机制对比

| 特性 | DQN | REINFORCE | A2C |
|------|-----|-----------|-----|
| **学习方式** | Off-policy | On-policy | On-policy |
| **更新时机** | 每步（mini-batch） | Episode结束 | 每n步 |
| **样本效率** | 高（可重用样本） | 低（样本只用一次） | 中等 |
| **方差** | 低 | 高 | 中等 |
| **偏差** | 可能有（函数近似） | 无偏 | 有（bootstrap） |
| **探索方式** | ε-贪婪 | 策略随机性 | 策略随机性+熵正则 |

---

## 3. 关键参数分析

### 3.1 DQN 关键参数

| 参数 | 推荐值 | 作用 | 影响 |
|------|--------|------|------|
| **learning_rate** | 1e-3 ~ 1e-4 | 网络学习速率 | 过大导致不稳定，过小收敛慢 |
| **gamma (γ)** | 0.99 | 折扣因子 | 影响对未来奖励的重视程度 |
| **epsilon_start** | 1.0 | 初始探索率 | 初期探索程度 |
| **epsilon_end** | 0.01 | 最终探索率 | 后期探索程度 |
| **epsilon_decay** | 0.995 | 探索衰减率 | 探索到利用的转换速度 |
| **batch_size** | 32~64 | 批量大小 | 影响梯度估计稳定性 |
| **buffer_size** | 10000+ | 回放缓冲区大小 | 影响样本多样性 |
| **target_update** | 100~1000步 | 目标网络更新频率 | 影响训练稳定性 |

### 3.2 REINFORCE 关键参数

| 参数 | 推荐值 | 作用 | 影响 |
|------|--------|------|------|
| **learning_rate** | 1e-3 ~ 3e-3 | 策略网络学习率 | 过大导致策略震荡 |
| **gamma (γ)** | 0.99 | 折扣因子 | 影响回报计算 |
| **use_baseline** | True | 是否使用基线 | **显著减少方差** |
| **hidden_size** | 128~256 | 隐藏层大小 | 影响网络表达能力 |

**基线(Baseline)的重要性**：
```
无基线：∇J = E[G_t * ∇log π]     → 高方差
有基线：∇J = E[(G_t - b) * ∇log π] → 低方差，无偏
```

### 3.3 A2C 关键参数

| 参数 | 推荐值 | 作用 | 影响 |
|------|--------|------|------|
| **lr_actor** | 1e-3 | Actor学习率 | 策略更新速度 |
| **lr_critic** | 1e-3 | Critic学习率 | 价值估计准确性 |
| **gamma (γ)** | 0.99 | 折扣因子 | 影响回报计算 |
| **n_steps** | 5~20 | n-step回报步数 | 偏差-方差权衡 |
| **entropy_coef** | 0.01~0.02 | 熵正则化系数 | **鼓励探索，防止策略崩溃** |
| **value_loss_coef** | 0.5 | 价值损失权重 | Actor/Critic平衡 |
| **max_grad_norm** | 0.5 | 梯度裁剪阈值 | 防止梯度爆炸 |

**n_steps 参数分析**：
```
n_steps=1:  R = r + γV(s')           → 低方差，高偏差（TD(0)）
n_steps=∞:  R = Σγ^t r_t            → 高方差，无偏（蒙特卡洛）
n_steps=5~20: 平衡偏差和方差         → 推荐选择
```

**熵正则化的作用**：
```
无熵正则：策略可能快速收敛到确定性策略，失去探索能力
有熵正则：L = L_actor - β * H(π)，鼓励策略保持随机性
```

---

## 4. 实验结果对比

### 4.1 CartPole-v1 训练结果

| 算法 | 达到475分所需Episodes | 最终评估平均分 | 训练稳定性 |
|------|----------------------|---------------|-----------|
| **DQN** | ~300-400 | 500 | 稳定 |
| **REINFORCE** | ~589 | 500 | 中等（有波动） |
| **A2C** | ~1353 | 500 | 需要调参 |

### 4.2 训练曲线特点

```
分数
500 ┤                              ╭────────── DQN
    │                        ╭────╯
400 ┤                   ╭────╯        ╭────── REINFORCE
    │              ╭────╯        ╭────╯
300 ┤         ╭────╯        ╭────╯
    │    ╭────╯        ╭────╯            ╭── A2C
200 ┤────╯        ╭────╯            ╭────╯
    │        ╭────╯            ╭────╯
100 ┤   ╭────╯            ╭────╯
    │───╯            ╭────╯
  0 ┼────────────────────────────────────────────→ Episodes
    0    200   400   600   800   1000  1200  1400
```

### 4.3 各算法优劣势总结

| 算法 | 优势 | 劣势 |
|------|------|------|
| **DQN** | 样本效率高；稳定；适合离散动作 | 需要额外内存存储回放；不适合连续动作 |
| **REINFORCE** | 实现简单；理论基础清晰；无偏估计 | 高方差；样本效率低；需要完整episode |
| **A2C** | 方差较低；可每步更新；适合连续动作 | 需要同时训练两个网络；参数敏感 |

---

## 5. 非DQN变种对比分析

### 5.1 A2C变种：加入熵正则化的改进

我们实现了一个A2C变种，主要改进包括：

#### 改进1：熵正则化

**原始A2C损失**：
```python
loss = actor_loss + value_loss_coef * critic_loss
```

**改进后A2C损失**：
```python
loss = actor_loss + value_loss_coef * critic_loss - entropy_coef * entropy
```

**效果对比**：

| 配置 | 训练结果 | 说明 |
|------|----------|------|
| 无熵正则 (entropy_coef=0) | 策略崩溃，卡在~9分 | 策略快速变成确定性，失去探索 |
| entropy_coef=0.01 | 缓慢收敛 | 探索不足 |
| **entropy_coef=0.02** | **成功达到500分** | 平衡探索与利用 |
| entropy_coef=0.1 | 收敛慢，不稳定 | 探索过度 |

#### 改进2：优势函数归一化

**原始优势计算**：
```python
advantage = returns - values
```

**改进后优势计算**：
```python
advantage = returns - values
advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)
```

**效果**：减少梯度方差，稳定训练。

### 5.2 REINFORCE vs A2C 详细对比

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    REINFORCE vs A2C 对比分析                             │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  特性              │  REINFORCE        │  A2C                           │
│  ─────────────────┼───────────────────┼─────────────────────────────── │
│  价值估计          │  蒙特卡洛回报 G_t │  Critic网络 V(s)               │
│  更新频率          │  Episode结束      │  每n步                         │
│  方差              │  高               │  中（使用Critic减少）          │
│  偏差              │  无               │  有（Critic近似引入）          │
│  网络数量          │  1个              │  2个（Actor + Critic）         │
│  探索机制          │  策略随机性       │  策略随机性 + 熵正则化         │
│  实现复杂度        │  简单             │  中等                          │
│  样本效率          │  低               │  中                            │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 5.3 关键代码差异

**REINFORCE 核心更新**：
```python
# 计算蒙特卡洛回报
returns = []
G = 0
for r in reversed(rewards):
    G = r + gamma * G
    returns.insert(0, G)

# 使用基线减少方差
baseline = np.mean(returns)
advantages = returns - baseline

# 策略梯度更新
policy_loss = -(log_probs * advantages).mean()
```

**A2C 核心更新**：
```python
# 使用Critic估计价值
values = critic(states)

# 计算n-step回报
returns = rewards + gamma * next_values * (1 - dones)

# 优势函数
advantages = returns - values.detach()

# Actor-Critic联合损失
actor_loss = -(log_probs * advantages).mean()
critic_loss = F.mse_loss(values, returns)
entropy_loss = -entropy.mean()
total_loss = actor_loss + 0.5 * critic_loss - 0.02 * entropy_loss
```

---

## 6. 结论与建议

### 6.1 算法选择建议

| 场景 | 推荐算法 | 理由 |
|------|----------|------|
| **简单离散动作任务** | DQN | 稳定、样本效率高 |
| **需要理论保证** | REINFORCE | 无偏估计，理论清晰 |
| **复杂任务/连续动作** | A2C | 平衡方差和偏差，可扩展性好 |
| **样本有限** | DQN | 经验回放提高样本利用率 |
| **在线学习** | A2C | 无需存储历史数据 |

### 6.2 调参建议

1. **DQN调参优先级**：
   - learning_rate > epsilon_decay > batch_size > buffer_size

2. **REINFORCE调参优先级**：
   - use_baseline > learning_rate > hidden_size

3. **A2C调参优先级**：
   - **entropy_coef** > n_steps > learning_rate > value_loss_coef

### 6.3 关键发现

1. **熵正则化对A2C至关重要**：没有熵正则化，A2C容易策略崩溃
2. **基线对REINFORCE至关重要**：显著减少方差，加速收敛
3. **DQN最稳定但需要额外内存**：经验回放带来稳定性但增加内存开销
4. **n-step参数影响偏差-方差权衡**：n=5~10通常是较好的选择

### 6.4 未来改进方向

- **PPO (Proximal Policy Optimization)**：限制策略更新幅度，更稳定
- **SAC (Soft Actor-Critic)**：最大熵框架，自动调节探索
- **TD3 (Twin Delayed DDPG)**：双Q网络减少过估计

---

## 附录：实验环境

- **环境**: CartPole-v1 (Gymnasium)
- **目标分数**: 475 (100 episode平均)
- **最高分数**: 500 (episode最大长度)
- **框架**: PyTorch 2.x
- **硬件**: CPU训练

---

*报告生成时间: 2024年*
